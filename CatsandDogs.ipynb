{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "0D85rnQ1V6jW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uc2GYRZzKyzh",
        "outputId": "b15dea6d-8f0e-46ac-8690-68e6af23a02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-06 21:10:41--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.63.207, 142.250.31.207, 142.251.111.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.63.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   172MB/s    in 0.4s    \n",
            "\n",
            "2024-11-06 21:10:42 (172 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the Data into arrays"
      ],
      "metadata": {
        "id": "xgEKdtTjV8Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "cats_dir = os.path.join(train_dir, \"cats\")\n",
        "dogs_dir = os.path.join(train_dir, \"dogs\")\n",
        "\n",
        "# Initialize as lists\n",
        "x_train = []\n",
        "y_train = []\n",
        "NUMBER_OF_EXAMPLES = 1000\n",
        "\n",
        "i = 0\n",
        "while i < NUMBER_OF_EXAMPLES:\n",
        "    if i % 2 == 0:\n",
        "        im = Image.open(os.path.join(cats_dir, os.listdir(cats_dir)[i])).convert(\"RGB\")\n",
        "        im_resized = im.resize((150, 150))\n",
        "        x_train.append(np.array(im_resized))\n",
        "        y_train.append(1)\n",
        "    else:\n",
        "        im = Image.open(os.path.join(dogs_dir, os.listdir(dogs_dir)[i])).convert(\"RGB\")\n",
        "        im_resized = im.resize((150, 150))\n",
        "        x_train.append(np.array(im_resized))\n",
        "        y_train.append(0)\n",
        "    i += 1\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "QOhkfFT1K4o1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beginning to define the model (this is where you come in, I loaded the pretrained model for you)"
      ],
      "metadata": {
        "id": "t7R64c0tV-zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "pretrained_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    input_shape=(150, 150, 3),\n",
        "    pooling='avg',\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Freeze the convolutional base\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "# Add custom layers on top\n",
        "model = models.Sequential()\n",
        "model.add(pretrained_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n"
      ],
      "metadata": {
        "id": "7zGbGnknK6uG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89aa310b-c5b3-44a5-bd6e-6ad544e41be4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=2e-5),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czm4NENjVhJv",
        "outputId": "3458535a-ea13-4770-fac1-6a48381d50c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 3s/step - accuracy: 0.9759 - loss: 0.0749 - val_accuracy: 0.9800 - val_loss: 0.0811\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - accuracy: 0.9713 - loss: 0.0859 - val_accuracy: 0.9800 - val_loss: 0.0786\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.9818 - loss: 0.0654 - val_accuracy: 0.9600 - val_loss: 0.0878\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.9808 - loss: 0.0684 - val_accuracy: 0.9800 - val_loss: 0.0771\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 3s/step - accuracy: 0.9897 - loss: 0.0561 - val_accuracy: 0.9750 - val_loss: 0.0794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_train, y_train)\n",
        "print(f'Validation accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKJNftkwVmIt",
        "outputId": "a14180fa-3970-4558-d9cb-52862f7e638f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.9904 - loss: 0.0509\n",
            "Validation accuracy: 0.9869999885559082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load an image for prediction\n",
        "img_path = 'cat.jpg'  # Update this path to the actual image file\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array /= 255.\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(img_array)\n",
        "print('Cat' if prediction < 0.5 else 'Dog')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPQ3g_QxVntD",
        "outputId": "77430fc3-202f-4bba-a411-b05cf19b4bac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 52s/step\n",
            "Dog\n"
          ]
        }
      ]
    }
  ]
}